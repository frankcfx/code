SparkProject
========================================
http://www.ibeifeng.com/goods-582.html
https://tech.meituan.com/tag/Spark
https://tech.meituan.com/spark-in-meituan.html

========================================
学习大数据项目/规划大数据项目/面试大数据项目
  -1. 了解项目的数据流程
  -2. 按照模块/指标进行一个划分，对划分之后的模块或者指标的功能、目标、产生的效益以及运行的方式
  -3. 项目/模块的核心技术点或者代码块
  总结：流程图、各个模块的功能/需要注意的事项、项目应用的核心技术点

批处理		交互式处理		流式数据处理		机器学习/数据挖掘/自然语言
SparkCore	SparkSQL		SparkStreaming		SparkMLlib/Graphx/Core
MapReduce	Hive			Storm				Mahout
Hive		Impala/Presto
Flink		Flink			Flink

分布式的文件系统：HDFS
集群管理系统：YARN


数据收集
  自己公司的数据：
    关系型数据库中的业务数据：
	  Sqoop、SparkSQL
	用户行为数据/非业务数据：
	  Flume、Kafka
  第三方的数据：
    爬虫
	
========================================
美团是数据驱动的互联网服务，用户每天在美团上的点击、浏览、下单支付行为都会产生海量的日志，这些日志数据将被汇总处理、分析、挖掘与学习，为美团的各种推荐、搜索系统甚至公司战略目标制定提供数据支持。
  数据来源：
    用户行为数据：
    	用户每天在美团上的点击、浏览、下单支付行为都会产生海量的日志
    业务数据库数据：
	    会员数据、商品数据、商家数据等等
  操作：
    汇总处理  => 数据收集+ETL
	分析 => 基于业务的一个统计(一般为Count类型的业务开发)
	挖掘与学习 => 机器学习
  作用/功能：
    为美团的各种推荐、搜索系统甚至公司战略目标制定提供数据支持

发展的阶段
  -01. HiveQL + MapReduce
    问题：
	  --1. 底层是MapReduce计算引擎，执行速度慢，对于DAG类型的的任务支持不太好(job与job之间的数据需要落磁盘)
	  --2. Hive没法对太复杂的业务进行处理，对于复杂的任务可能需要编写MapReduce程序或者采用Hive+Python的形式来进程处理。对于工程师要求相对来讲比较高
  -02. Spark
    效果：
	  Spark作业数和MapReduce作业数比例为4：1
	  在相同的资源使用情况下，作业执行速度提升了十倍
========================================
项目：基于Spark的交互式的用户行为分析系统

用于提供对海量的流量数据进行交互式分析的功能，系统的主要用户为公司内部的PM和运营人员
### 数据
  海量的流量数据 ==> 用户行为数据
### 用户/操作者
  PM(产品经理)和运营人员

需求：
  PM以及运营人员除了查看一些聚合指标以外，还需要根据自己的需求去分析某一类用户的流量数据，进而了解各种用户群体在App上的行为轨迹
	1. 自助查询，不同的PM或运营人员可能随时需要执行各种各样的分析功能，因此系统需要支持用户自助使用。
	2. 响应速度，大部分分析功能都必须在几分钟内完成。
	3. 可视化，可以通过可视化的方式查看分析结果。(echarts、highcharts...)
问题：
	1. 海量数据的处理，用户的流量数据全部存储在Hive中，数据量非常庞大，每天的数据量都在数十亿的规模。
	2. 快速计算结果，系统需要能够随时接收用户提交的分析任务，并在几分钟之内计算出他们想要的结果。
技术的选型：
  1. MapReduce
  2. Spark（选择）
功能：
  PM可以优化产品设计，运营人员可以为自己的运营工作提供数据支持
效果：
  90%的Spark作业运行时间都在5分钟以内，剩下10%的Spark作业运行时间在30分钟左右，该速度足以快速响应用户的分析需求。
  目前每个月该系统都要执行数百个用户行为分析任务，有效并且快速地支持了PM和运营人员的各种分析需求

=======================================================
集群规模
  考虑的指标：机器的数量、磁盘、内存、cpu、网络
  备注：不管什么集群的指标实质上都是和业务数据以及Job数量挂钩的
  机器数量：
    小规模集群：5~11台
	中：20~50台
	大：100台+
  磁盘：
    -1. 每台机器挂载几个磁盘
	  一般情况下数据和日志分别存储不同的磁盘
	-2. 每个磁盘的空间大小(数据存储机器)
	  考虑整个集群的磁盘空间大小 / 集群的机器数量就可以磁盘的空间大小
	  整个集群的磁盘空间大小是考虑最多允许存储多少数据，可能需要推测未来的数据量，eg：保存最近三年的数量，计算得出三年的总数据量大概是3T, 总磁盘数据存储大小需要: 3T * 3 = 9T, 总磁盘大小: 9 / 0.9 = 10T
	-3. 每个磁盘的空间大小(NameNode类型的元数据存储机器)
	  磁盘一般为500G
	-4. 磁盘是否需要做RAID备份阵列
	  NameNode类型的元数据存储机器的磁盘需要做
	  DataNode类型的数据存储机器的磁盘不需要做
	-5. 磁盘IO
  内存：
    如果使用Impala/Presto等基于内容的计算框架/执行引擎的话，内存至少128G+
	如果使用CM(cloudera manager)/Ambari服务，那么server所在的机器至少32G+，agent没有要求
	NN/HMaster/RM：
	  16G
	DN/HRegionServer/NM:
	  系统&进程需要的内存：16G(HRegionServer至少8G+)
	  Task任务的运行内存：
	    只运行MapReduce：
		  8G+
		运行MapReduce、Spark、Storm等:
		  16G+
  CPU:
    非运算节点：2~8核
    数据运算节点:
      至少4核
      一般情况下，CPU和内存的比例为1:4或者1:8, eg: 内存为32G，cpu可以是8核或者4核	  
  网络：
    在一个局域网中最好
	网络最好是千兆或者万兆

===================================================
项目结构
  -1. 数据收集系统
    用户行为数据
	  定义：用户在系统上操作产生的数据，比如：点击、浏览、收藏、下单等等
	  埋点 -> 用户行为处罚数据的发送操作 -> 将数据发送到日志服务器 -> Nginx日志服务器以日志的形式将数据保存文件 -> Flume监控文件数据并将数据进行收集传输 -> Flume/Kafka -> ETL -> Hive表
	业务数据库的数据(RDBMs)
	  利用SQoop或者SparkSQL完成同步操作
  -2. 交互式的用户行为分析系统
1. 用户在系统界面中选择某个分析功能对应的菜单，并进入对应的任务创建界面，然后选择筛选条件和任务参数，并提交任务。
2. 由于系统需要满足不同类别的用户行为分析功能（目前系统中已经提供了十个以上分析功能），因此需要为每一种分析功能都开发一个Spark作业。 ==> 一个分析功能就是一个Spark作业的执行模板程序
3. 采用J2EE技术开发了Web服务作为后台系统，在接收到用户提交的任务之后，根据任务类型选择其对应的Spark作业，启动一条子线程来执行Spark-submit命令以提交Spark作业。 ==> 接收到数据后，需要将数据/任务保存数据库，然后根据任务id进行对应spark作业的启动 ===> J2EE系统如何启动spark应用???(并且传递一个任务id)
4. Spark作业运行在Yarn集群上，并针对Hive中的海量数据进行计算，最终将计算结果写入数据库中。===> Spark作业如何获取任务的过滤参数?? ==> spark作业可以通过传递过滤的任务id从数据库获取任务过滤参数
5. 用户通过系统界面查看任务分析结果，J2EE系统负责将数据库中的计算结果返回给界面进行展现。 ==> J2EE系统如何知道Spark应用执行完成???  ==> 可以在J2EE系统中编写代码，通过spark的job history server提供的rest api获取数据，然后根据执行历史数据判断任务是否执行完成

注意：
  在考虑项目结构是否合理的时候，首相需要考虑的是该技术能否实现业务需求，再考虑结构是否具有容错性、扩展性、可靠性、高性能....

=======================================
Spark应用的启动方式
  bin/spark-submit --master yarn xxxx  yyy.jar zzz
J2EE系统如何启动spark应用
  -> Java代码如何启动spark应用
    -> Java代码如何调用shell脚本
  -1. java代码直接调用本地的spark-submit脚本程序
    java代码中调用本地的shell脚本来启动spark应用，脚本中是spark的启动命令
	优点：
	  简单，直接使用java的Runtime类即可完成
	缺点：
	  需要将shell脚本copy到所有可能运行的J2EE的服务器上
	  在所有的机器上(脚本所在机器)需要安装配置spark-submit的执行提交环境(spark的spark-shell的执行本地环境&yarn集成)以及jar包
  -2. Java代码调用远程的spark-submit脚本程序
    Java通过ssh来远程启动shell脚本，脚本中是spark的启动命令
	优点：
	  简单，而且不需要在J2EE系统上配置安装spark的提交环境
	缺点：
	  需要使用第三方的jar文件(ssh框架), eg：jsch
	  代码中需要考虑远程防止spark-submit机器宕机情况的解决方案
  -3. 直接在java代码中启动spark应用程序
    直接运行spark应用，并且给定相关的参数
	参考：
      Day20[20171118]_SparkCore（二）\06_参考资料\Windows环境中MR任务的三种运行方式.zip
	  压缩包中的: MR任务的三种运行方式.docx
	  第二种运行方式(本地提交&集群运行)
	-a. 所有可能运行的机器安装HADOOP(hadoop的提交客户端)和Spark(Spark local模式)，并且配置HADOOP_HOME
	-b. 将hadoop的所有配置文件(core-site.xml、yarn-site.xml、hdfs-site.xml)添加到项目的classpath中
	-c. 添加相关的配置项(代码中添加)
	  val conf = new SparkConf()
	    .setMaster("yarn")
		.setAppName("xxx")
		.set("xxxx", "xxx")
		.set("xxxx", "xxx")
		.setJars(Array("/本地的jar文件(即spark应用打包形成的jar文件)"))
    -d. 将spark应用打包(形成jar文件)
    -e. java代码直接调用spark应用的main方法来执行即可
        eg: PVAndUVSparkCore.main(args);	
================================================
Task核心抽象
  每个任务都是一个Task对象，对应到数据库来讲其实就是一条记录，使用一个唯一的id来表示数据
  每个任务使用一个json字符串来表示任务的过滤参数，数据库的字段类型是Text

交互式的概念：
  前端创建一个任务，后台立即就将该任务添加到执行列表中，执行完成后，将执行结果保存到数据中，同时该任务的结果会反馈给前端页面 ==> 这个过程就叫做交互式
  前端、后台、spark应用之间传递信息主要来讲就是任务id

================================================
项目模块
  -1. 会话分析(session分析) => SparkCore
  -2. 跳出率分析 => SparkCore
  -3. 各个区域热门商品统计 => SparkSQL
  -4. 广告流量的实时统计 => SparkStreaming

================================================
模块一：会话分析
  会话(session)概念：当用户访问系统的时候，就产生一个会话，当离开的时候，该会话就结束；会话使用一个唯一id来表示，在一个会话中有可能会产生多条数据(>=1)
  会话是如何区分???会话使用一个唯一的id来表示，当第一次进入系统的时候，产生一个唯一id，当离开的时候，该唯一id被删除  
  涉及到的数据：
    -1. 用户行为数据
	  ETL操作之后的保存在Hive表中的用户行为数据(分区表，一天一个分区)
	  表名称：user_visit_action
	  涉及到的字段：
	    date: String
		userId: Long
		sessionId: String
		pageId: Long
		actionTime: String
		searchKeyword: String
		clickCategoryId: String
		clickProductId: String
		orderCategoryIds: String
		orderProductIds: String
		payCategoryIds: String
		payProductIds: String
		cityId: Int
	-2. 用户/会员信息表
	  表名称：user_info
	  基于业务系统中的会员数据进行扩充之后得到的一张表
	  数据存储在Hive中
	  字段：
	    userId: String
		userName: String
		name:String
		sex:String
		age:Int
		professional: String
  
  数据处理过程：
    -1. 将数据按照session id进行数据的聚合预处理操作
	  --a. 根据传入的任务id获取任务参数
	  --b. 创建SparkContext以及SQLContext等上下文对象
	  --c. 根据过滤参数读取Hive表中的数据并进行数据过滤操作
	  --d. 对过滤之后的数据进行数据聚合操作
	-2. 具体的业务指标的计算
      --a. 需求一：用户访问session的基本聚合统计
基本概念：
  会话数量：有一个会话就算一个会话
  会话长度：一个会话中，最后一条会话记录的时间减去第一条会话记录的时间就当做会话的长度  
指标：
  总会话数量、总会话的长度、总无效会话的数量(会话长度小于1s的会话数量)
  分访问时长区间来统计各个区间的会话数量
0~3s/4~10s/11~30s/31~60s/1~3min/3~6min/6min+
  分时间段/小时段来统计各个区间段的总会话数量、总会话长度、总无效会话的数量
0~1h/1~2h/..../9~10h/...../22~23h/23~0h
  分pv的区间来计算各个区间的总会话数量、总会话长度
1pv/2~3pv/4~6pv/7~10pv/11~20pv/20~60pv/60pv+

      --b. 需求二：按照时间比例抽取具体session的访问记录
需求查看数据的特征/分布情况/用户的访问路径 -> 拿所有的数据来看，不现实的 -> 从数据中随机的抽取一部分数据来体现整体的数据特征 -> 抽取数据的过程中需要注意公平而随机
过程：
  --1. 计算总的会话数量以及各个小时区间段的会话数量
  --2. 计算总的需要抽取的会话数量以及各个小时区间段的会话数量
  --3. 对每个小时区间段抽取数据
  --4. 对每个小时区间段的数据进行检查，并补充抽取数据
  --5. 抽取结果保存HDFS、MySQL
功能：可以让PM获取用户的访问路径，理解用户的操作习惯，可以针对性的进行一些优化操作

      --c. 需求三：获取下单、支付、点击行为次数最多的前10个品类 => 分组排序TopN
每个会话中都有可能产生点击、下单、支付行为(>=0)
数据不需要去重的，有一条算一条
过程：
  -1. 获取每个会话中每个品类的点击次数、下单次数、支付次数
会话1 品类1 点击次数 下单此时 支付次数
会话1 品类2 点击次数 下单此时 支付次数
会话1 品类3 点击次数 下单此时 支付次数
会话2 品类1 点击次数 下单此时 支付次数
会话3 品类3 点击次数 下单此时 支付次数
  -2. 数据的聚合
品类1 点击次数 下单此时 支付次数
品类2 点击次数 下单此时 支付次数
品类3 点击次数 下单此时 支付次数
  -3. 聚合结果获取Top10结果
    按照操作类型分组，然后每组中获取值最大的前10个数据
  -4. Top10结果保存数据库
  
      --d. 需求四：获取Top10的点击品类对应的session的记录
-1. 利用需求三的结果
-2. 获取这些品类出现的session的数据
-3. 对这些session数据计算每个session中的记录数量
-4. 获取数量最多的前10个session	  

  测试运行
    -1. 修改usertrack.properties中的参数，修改jdbc的相关连接参数
	-2. 在mysql数据库中执行usertrack.sql文件中的代码
	  Day24[20171202]_SparkProject(一)\05_随堂代码\usertrack.sql
	-3. 修改tb_task中记录的任务过滤参数字段中的时间属性
	-4. 编写一个main方法
	-5. 右击选择运行即可






















	  